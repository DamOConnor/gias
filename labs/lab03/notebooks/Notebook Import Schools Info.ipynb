{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0d4dba-8837-4fd6-900e-e93c12f55cab",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Import Schools Data\n",
    "Import data from https://get-information-schools.service.gov.uk/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2561d-fcb9-44c0-814c-827b025736c5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c1fa3-0de2-4e10-a50f-4d1a32fa5488",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852e997-9d4f-4d9e-a9e7-fa67fbab9b22",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "pFilepath = \"Files\"\n",
    "debug_yn = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba985047-ef97-4fa5-8155-af28e1c249a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18030704-616b-4f1c-a732-c4c6270e6811",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from notebookutils import mssparkutils\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171a398-8f94-413d-b745-a4556cd0468e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596e629-3885-4a6a-b260-9c2dd9dc196d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# More Pythonic method but ended up with duplicate columns\n",
    "# Function to clean up column names\n",
    "def clean_column_names(df: DataFrame) -> DataFrame:\n",
    "    # Create a mapping of original column names to cleaned column names: \\W+ matches one or more non-word characters (equivalent to [^a-zA-Z0-9_])\n",
    "    cleaned_columns = {col: re.sub(r'\\W+', '_', col).strip('_').lower() for col in df.columns}\n",
    "\n",
    "    # Rename columns using the mapping\n",
    "    return df.toDF(*[cleaned_columns[col] for col in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102a8e2-66f1-4506-a675-4e4f13fc6ebe",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def split_date_from_filename(table_name):\n",
    "    \"\"\"\n",
    "    Cleans the table name by removing the date in yyyyMMdd format if present at the end.\n",
    "\n",
    "    :param table_name: The original table name.\n",
    "    :return: The cleaned table name with the date removed.\n",
    "    \"\"\"\n",
    "    # Regular expression to match the date in the format yyyyMMdd at the end of the table name\n",
    "    cleaned_name = re.sub(r'\\d{8}$', '', table_name)\n",
    "    \n",
    "    # Optionally, can clean up any trailing underscores or other characters\n",
    "    cleaned_name = cleaned_name.strip('_').lower()\n",
    "    \n",
    "    return cleaned_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb06dbc-bedd-4779-ac2b-11551f81fc61",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f4bbd-477d-41b5-904f-e3c1e1442587",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Drop existing tables\n",
    "Drops all existing tables in the Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13640783-f16d-45d3-808b-fb770cc22711",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SHOW TABLES\")\n",
    "display(df)\n",
    "\n",
    "# Drop all tables\n",
    "for table in df.select(\"tableName\").collect():\n",
    "    try:\n",
    "        if debug_yn:\n",
    "            print(f\"Would drop table: {table['tableName']}\")\n",
    "        else:\n",
    "            spark.sql(f\"DROP TABLE {table['tableName']}\")\n",
    "            print(f\"Dropped table: {table['tableName']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to drop table: {table['tableName']} due to error: {e}\")\n",
    "        \n",
    "print(\" \")\n",
    "print(\"---- DROP TABLES END ----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe7992-9fd3-49cd-bd34-0e45394f6fa4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Get List of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918e84f-d404-49f0-8444-c533c6b19d91",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Get list of files to load - can't use wildcards here\n",
    "files = mssparkutils.fs.ls(f\"{pFilepath}\")\n",
    "\n",
    "# Filter the list to include only CSV files\n",
    "csv_files = [file for file in files if file.name.endswith('.csv')]\n",
    "\n",
    "print(f\"Total no. of files: {len(files)} -> Filtered files: {len(csv_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03621e7-bb98-4c1b-a714-3880ed2768fe",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb3a7da-8151-4535-a097-3811bd0c2bb0",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"5d3b2799-cc52-4a8a-8c39-0443f2a91074\",\"activityId\":\"cba401c0-961c-438f-8aa6-49f9cf31292d\",\"applicationId\":\"application_1736421625128_0001\",\"jobGroupId\":\"9\",\"advices\":{\"info\":1}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Loop through the files and load them (serial)\n",
    "for file in csv_files:\n",
    "    # Remove the .csv element from filename to make a clean table name\n",
    "    clean_tablename = file.name.split('.csv')[0]\n",
    "    \n",
    "    # Split date from filename\n",
    "    clean_tablename = split_date_from_filename(clean_tablename)\n",
    "\n",
    "    print(f\"Processing file: {file.name} -> Clean table name: {clean_tablename}\")\n",
    "  \n",
    "    # Load the table to a dataframe\n",
    "    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"{pFilepath}/{file.name}\")\n",
    "    \n",
    "    # Clean up the column names\n",
    "    df = clean_column_names(df)\n",
    "\n",
    "    # Save to the database\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(f\"Tables/{clean_tablename}\")\n",
    "\n",
    "print(\" \")\n",
    "print(\"---- LOAD TABLES END ----\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "51f4fcec-f49d-41a3-9972-a58e4b407fc6",
    "default_lakehouse_name": "lh_schools_doc",
    "default_lakehouse_workspace_id": "51053dbe-085c-4fa4-bde3-613c77b7db2e"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {
    "c5c40f9a-aef8-4e14-9f6e-505bd44c49c7": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "count",
        "binsNumber": 10,
        "categoryFieldKeys": [
         "0"
        ],
        "chartType": "bar",
        "evaluatesOverAllRecords": false,
        "isStacked": false,
        "seriesFieldKeys": [
         "0"
        ],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [],
       "schema": [
        {
         "key": "0",
         "name": "namespace",
         "type": "string"
        },
        {
         "key": "1",
         "name": "tableName",
         "type": "string"
        },
        {
         "key": "2",
         "name": "isTemporary",
         "type": "boolean"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "candidateVariableNames": [
        "df"
       ],
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
